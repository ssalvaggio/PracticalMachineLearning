Training data
========================================================
Practical Machine Learning - Course Project - Salvino A. Salvaggio - July 2014

```{r global_options, include=FALSE}
library('knitr')
opts_chunk$set(warning=FALSE, message=FALSE)
```

### Executive Summary
This work builds a machine learning algorithm to predict activity quality from activity monitors.
After the initial loading and exploration of the dataset, I removed several variables which are empty or NAs and reduced the original dataset from 160 to 53 variables (52 predictors + variable 'classe' as outcome). Then, a sampled subset (40% out of 19,622 observations) of the reduced dataset was partioned in 2 equal parts for **training** and **cross-validation**. Five (5) different machine learning algorithms were run on the training dataset, next the models were cross-validated applying them to the validation dataset. At each stage (training and validation), accuracy and error of the models were measured and compared to identify the model with the highest prediction rate. In-sample and out-sample errors rates ultimately indicate that random forest (RF) algorithm is the most accurate in predicting. Finally, **the RF model was applied to the 20-case test dataset and accurately predicted all the cases**.

<u>Please, note</u>: In order to improve readability of this page, most of the R code chunks were given the parameter `echo=FALSE`

### Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

* More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset)
* The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).
* The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).
* The data for this project come from this [source](http://groupware.les.inf.puc-rio.br/har). 

### Load files and preprocess data
```{r LoadFiles, results='hide', cache=TRUE}
library('doMC') # load library doMC that will force R to use all cores of Mac in parallel
registerDoMC(cores = 2)
set.seed(3846)
test <- data.frame(read.csv('pml-testing.csv', header = T))
data <- data.frame(read.csv('pml-training.csv', header = T))
dim(data)
str(data)
head(data)
```
Many variables in the original data set are mostly populated with NAs or left empty. Removing them does not impact much machine learning algorithms.

```{r InitialClean}
library('plyr') ; library('caret')
# populate empty cells with NAs
data[data == ''] <- NA
# define function to count NAs
nmissing <- function(x) sum(is.na(x))
# use colwise from plyr package to apply the function to columns
missingNA <- colwise(nmissing)(data)
# identify columns that have more than 50% NAs
indexMissingNA <- missingNA >= dim(data)[1]/2
# clean data base on such index
data <- data[, !indexMissingNA]
# manually remove user_name, timestamps
data <- data[, -(1:7)]
```

The outcome of this initial cleaning phase is a data set reduced from 160 to 53 variables (52 predictors and the variable 'classe'). Reasons for removing the variables:

* many variables are empty or populated with NAs
* I decided to remove all variables which are not the outcome of a sensor (timestamps, user name)
* 'window' variables are also removed, for they seem to mimic the behavior of the 'classe' variable (almost full correlation)

### Data Partition

The rationale behind the partition of the original data is as follows:

* A 20-case test set is already provided, there therefore is no need to generate a new test set
* The original training data set counts more than 19,000 observations --which in this case is more than sufficient to train and validate the ML algorithms. Fourty (40) percent of the observations will be randomly selected for that purpose
* this 40% subset will then be split in 2 equal parts for training and validation. Each of these subset will therefore count 20% of the original data set


```{r Sampling}
# sample half of original observations
sample <- data[sample(nrow(data), dim(data)[1]*0.4, replace=FALSE), ]

# create training and validation subsets
indexSample <- createDataPartition(y=sample$classe, p=.5, list=FALSE)
training <- sample[indexSample,]
validation <- sample[-indexSample,]
```

#### Identify and plot the most correlated variables
As part of the initial data exploration I wanted to identify and plot the variables which have at least 0.9 correlation.
```{r PlotCorrelated, cache=TRUE, echo=FALSE, fig.width=10, fig.height=10}
# list the variables which are strongly correlated
M <- abs(cor(training[,-53]))
diag(M) <- 0
strongCor <- which(M >= 0.9, arr.ind=TRUE)
strongCorVariables <- sort(unique(strongCor[,1]))
numStrongCor <- length(strongCorVariables)
correlated <- training[, c(strongCorVariables, 53)]
par(mar=c(2,2,2,2), cex.axis=0.8)
pairs(correlated, main='Most correlated variables', pch='.', col='steelblue')
```

### Machine learning

In this chapter I will:

1. run several ML algorithms (training)
2. validate models by predicting values in validation data set, then compare accuracy, in-sample and out-sample error rates

#### ML models

Four algorithms will be tested: trees, random forest, GBM, support vector machine (SVM)

Several tests showed that, in my case, time of calculation increases only by a couple of minutes when including all the 52 predictors instead of focusing on highly correlated variables only (MacBookPro 2014, 16GB RAM). However, whit a limited number of variables the accuracy of final testing drops. That is why, I have decided to run the ML models on all the predictors (52 variables). Machine Learning models tested are:

1. Trees (rpart method) -`fitTREE <- train(classe ~ ., data=training, method='rpart', ```)`
2. Random Forest - `fitRF <- train(classe ~ ., data=training, method='rf',  ...)`
3. GBM - `fitGBM <- train(classe ~ ., data = training, method = 'gbm', ...)`
4. Support Vector Machine (SVM) - `fitSVM <- train(classe ~., data=training, method = 'svmLinear', ...)`
5. Principal Components Analysis (PCA) - `fitPCA <- train(correlatedPCA$classe ~ ., method='glmnet', ...)`

```{r MLmodels, echo= FALSE, cache=TRUE}
library('randomForest')
# 1- Trees (rpart method)
fitTREE <- train(classe ~ ., data=training, method='rpart', preProcess=c("center", "scale"))

# 2- Random Forest
fitControlRF <- trainControl(method = "cv", number = 5)
fitRF <- train(classe ~ ., data=training, method='rf', preProcess=c("center", "scale"), 
               trControl = fitControlRF, prox=TRUE, verbose = FALSE)

# 3- GBM
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
fitGBM <- train(classe ~ ., data = training, method = 'gbm', trControl = fitControl, 
                verbose = FALSE)

# 4- Support Vector Machine (SVM)
fitSVM <- train(classe ~., data=training, method = 'svmLinear', trControl = fitControl)

# 5- Principal Components Analysis (PCA)
strongCorPCA <- which(M >= 0.7, arr.ind=TRUE)
strongCorVariablesPCA <- sort(unique(strongCorPCA[,1]))
numStrongCorPCA <- length(strongCorVariablesPCA)
correlatedPCA <- training[, c(strongCorVariablesPCA, 53)]
fitPCA <- train(correlatedPCA$classe ~ ., method='glmnet', preProcess='pca', 
                data=correlatedPCA)
```

<u>Side note</u>: PCA shows that it takes 23 variables to capture 95% of all variance.

```{r plotPCAcontribution, echo=FALSE, cache=TRUE, fig.width=8, fig.height=5}
PCAanalysis <- prcomp(training[,-dim(training)[2]], scale. = TRUE)
varianceVector <- PCAanalysis$sdev ^ 2
relativeVariance <- varianceVector / sum(varianceVector)
cumulativeVariance <- cumsum(relativeVariance)
plot(cumulativeVariance, type='s', main='Cumulative variance', 
     ylab='Sum variance (PCA)', xlab='Index variables', ylim=c(0,1), col='blue', 
     lwd=3)
abline(v=24, col='grey')
abline(h=0.95, col='grey')
```


#### Errors and cross-validation

The random forest (RF) algorithm gives the best prediction rate on both the training and the validation datasets. 

<u>Remark</u>: Although it is usually expected that the out-sample error rate is higher than the in-sample error rate it worths noting that in this case the out-sample accuracy of random forest (on validation dataset) is higher than the in-sample accuracy (on training dataset). This definitely is not common and should raise a flag for more attention to the way the RF model deals with noise.

GBM is the second most in-sample accurate predictive model (only approx. 2% less accurate than random forest), and its out-sample accuracy expectedly is lower than in-sample accuracy.


```{r ErrorsValidation, echo=FALSE}
method <- c('trees', 'RF', 'GBM', 'SVM', 'PCA')

# in-sample error
inSample.Accuracy <- c(max(fitTREE$results[,2]), 
                       max(fitRF$results[,2]), 
                       max(fitGBM$results[,4]),
                       max(fitSVM$results[,2]),
                       max(fitPCA$results[,3])
                       )
inSample.Error <- c(1 - max(fitTREE$results[,2]), 
                    1 - max(fitRF$results[,2]), 
                    1 - max(fitGBM$results[,4]),
                    1 - max(fitSVM$results[,2]),
                    1 - max(fitPCA$results[,3])
                    )

# TREE prediction on validation set
predictedValuesTREE <- predict(fitTREE, validation)
confM.TREE <- confusionMatrix(validation$classe, predictedValuesTREE)

# RF prediction on validation set
predictedValuesRF <- predict(fitRF, validation)
confM.RF <- confusionMatrix(validation$classe, predictedValuesRF)

# GBM prediction on validation set
predictedValuesGBM <- predict(fitGBM, validation)
confM.GBM <- confusionMatrix(validation$classe, predictedValuesGBM)

# SVM prediction on validation set
predictedValuesSVM <- predict(fitSVM, validation)
confM.SVM <- confusionMatrix(validation$classe, predictedValuesSVM)

# PCA prediction on validation set
predictedValuesPCA <- predict(fitPCA, validation) 
confM.PCA <- confusionMatrix(validation$classe, predictedValuesPCA)

outSample.Accuracy <- c(as.numeric(confM.TREE$overall[1]),
                        as.numeric(confM.RF$overall[1]),
                        as.numeric(confM.GBM$overall[1]),
                        as.numeric(confM.SVM$overall[1]),
                        as.numeric(confM.PCA$overall[1])
                        )
outSample.Error <- c(1 - as.numeric(confM.TREE$overall[1]),
                     1 - as.numeric(confM.RF$overall[1]),
                     1 - as.numeric(confM.GBM$overall[1]),
                     1 - as.numeric(confM.SVM$overall[1]),
                     1 - as.numeric(confM.PCA$overall[1])
                        )
errors.df <- data.frame(method = method, 
                        inSample.Acc = inSample.Accuracy, 
                        inSample.Err = inSample.Error, 
                        outSample.Acc = outSample.Accuracy, 
                        outSample.Err = outSample.Error, 
                        stringsAsFactors=FALSE)
errors.df
```

```{r, echo=FALSE}
declare <- NULL
declare <- rbind(declare, paste('Lowest in-sample error: ', round(as.vector(errors.df[which.max(errors.df$inSample.Acc), 2]), 4)*100, '% with method ', as.vector(errors.df[which.max(errors.df$inSample.Acc), 1]), '.\n', sep=''))
declare <- rbind(declare, paste('Lowest out-sample error: ', round(as.vector(errors.df[which.max(errors.df$outSample.Acc), 4]), 4)*100, '% with method ', as.vector(errors.df[which.max(errors.df$outSample.Acc), 1]), '.', sep=''))
cat(declare)
```


**Accuracy comparison plot**
```{r PlotAccuracy, echo=FALSE, fig.width=11, fig.height=5}
library('reshape2')
meltErrors <- melt(errors.df, id=c('method'))
ggplot(meltErrors, aes(x=variable, y=value, fill=method)) + 
    geom_bar(position='dodge', stat='identity') + 
    scale_fill_brewer(palette='Accent') + 
    ggtitle('Accuracy & Errors by ML Method') + 
    theme_bw() + 
    theme(panel.grid.major.x = element_blank(), 
          panel.grid.minor.x = element_blank(), 
          panel.grid.major.y = element_line(colour='grey60',
                                            linetype='dashed'))
```



### Predict test cases

Last but not least, the most accurate model (random forest) is applied to predict the final 20 test cases, and save each prediction in a txt file. Submission to the programming assignment for automated grading confirms 100% accuracy in predicting the 20 test cases.

```{r finalTest}
predictedTEST <- predict(fitRF, test)
predictedTEST
# Save txt files for assignment submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictedTEST)
```

